import os
from dotenv import load_dotenv
from azure.core.credentials import AzureKeyCredential
from azure.search.documents.indexes import SearchIndexerClient
from azure.search.documents.indexes.models import (
    SearchIndexer,
    SearchIndexerSkillset,
    SplitSkill,
    InputFieldMappingEntry,
    OutputFieldMappingEntry,
    FieldMapping,
    IndexingParameters,
    BlobIndexerDataToExtract
)

# Load environment variables
load_dotenv()

SERVICE_ENDPOINT = os.getenv("AI_SEARCH_ENDPOINT")
API_KEY = os.getenv("AI_SEARCH_API_KEY")
DATA_SOURCE_NAME = "companydata-blob"  # From your create_data_source.py
TARGET_INDEX_NAME = "kliuuuuuuuuuuuuuuutllllllllllllllllllliutyilyuliyuh"
SKILLSET_NAME = "chunking-skillset"
INDEXER_NAME = "new-company-indexer"

# Initialize the SearchIndexerClient
credential = AzureKeyCredential(API_KEY)
indexer_client = SearchIndexerClient(endpoint=SERVICE_ENDPOINT, credential=credential)

# 1. Create the Skillset with a SplitSkill
split_skill = SplitSkill(
    name="chunker",
    description="Split content into pages/chunks",
    context="/document",
    text_split_mode="pages",
    maximum_page_length=2000,  # Suitable for ada-002
    page_overlap_length=200,   # Some overlap can be beneficial
    inputs=[
        InputFieldMappingEntry(name="text", source="/document/content"),
    ],
    outputs=[
        OutputFieldMappingEntry(name="textItems", target_name="myTextChunks")
    ]
)

skillset = SearchIndexerSkillset(
    name=SKILLSET_NAME,
    skills=[split_skill],
    description="Skillset to chunk documents and generate embeddings"
)

try:
    print(f"Creating or updating skillset '{SKILLSET_NAME}'...")
    indexer_client.create_or_update_skillset(skillset)
    print(f"Skillset '{SKILLSET_NAME}' created or updated successfully.")
except Exception as e:
    print(f"Error creating/updating skillset: {e}")
    exit()

# 2. Define Field Mappings for the Indexer
#    - chunk_id: auto-generated by the service for one-to-many documents
#    - text_vector: populated by the index's integrated vectorizer on the 'chunk' field
#    - companyName: no mapping, will be null or handled separately
field_mappings = [
    FieldMapping(source_field_name="metadata_storage_path", target_field_name="parent_id"),
    FieldMapping(source_field_name="metadata_storage_name", target_field_name="title"),
    # This mapping is crucial for one-to-many document generation from chunks
    FieldMapping(source_field_name="/document/myTextChunks/*", target_field_name="chunk")
]

# 3. Create the Indexer
# Define the configuration for the indexer parameters as a dictionary
indexer_configuration_dict = {
    "indexedFileNameExtensions": ".pdf,.xml",  # Comma-separated string for REST API
    "dataToExtract": BlobIndexerDataToExtract.CONTENT_AND_METADATA
    # Add other blob-specific parameters here if needed, e.g.:
    # "parsingMode": "default",
}

# Create the IndexingParameters object
indexer_parameters_obj = IndexingParameters(
    configuration=indexer_configuration_dict
    # You can also set other top-level parameters like batch_size, max_failed_items here
    # batch_size=10, # Default for blobs is 10
    # max_failed_items=0,
    # max_failed_items_per_batch=0
)

indexer = SearchIndexer(
    name=INDEXER_NAME,
    description="Indexer to process company PDF and XML files, chunk them, and vectorize.",
    data_source_name=DATA_SOURCE_NAME,
    target_index_name=TARGET_INDEX_NAME,
    skillset_name=SKILLSET_NAME,
    field_mappings=field_mappings,
    parameters=indexer_parameters_obj, # Use the IndexingParameters object with dict config
    # Optional: Set a schedule
    # from azure.search.documents.indexes.models import IndexingSchedule # Import this if using schedule
    # schedule=IndexingSchedule(interval="PT2H") # e.g., run every 2 hours
)

try:
    print(f"Creating or updating indexer '{INDEXER_NAME}'...")
    indexer_client.create_or_update_indexer(indexer)
    print(f"Indexer '{INDEXER_NAME}' created or updated successfully.")
except Exception as e:
    print(f"Error creating/updating indexer: {e}")
    # Optionally, you might want to delete the skillset if indexer creation fails
    # try:
    #     indexer_client.delete_skillset(SKILLSET_NAME)
    #     print(f"Cleaned up skillset '{SKILLSET_NAME}' due to indexer creation failure.")
    # except Exception as se:
    #     print(f"Error cleaning up skillset: {se}")
    exit()

# 4. Optionally, run the indexer on-demand and check status
try:
    print(f"Running indexer '{INDEXER_NAME}'...")
    indexer_client.run_indexer(INDEXER_NAME)
    print(f"Indexer run initiated for '{INDEXER_NAME}'.")

    import time
    print("Waiting for 30 seconds before checking indexer status to allow some processing time...")
    time.sleep(30) 
    
    status = indexer_client.get_indexer_status(INDEXER_NAME)
    print(f"Indexer '{INDEXER_NAME}' current status overview:")
    print(f"  Overall status: {status.status}")

    # Details from the 'last_result' (reflects the ongoing or last completed operation)
    if status.last_result:
        print(f"  Last Result (ongoing or last completed operation details):")
        print(f"    Status: {status.last_result.status}")
        
        items_processed = getattr(status.last_result, 'items_processed', 'N/A')
        items_failed = getattr(status.last_result, 'items_failed', 'N/A')
        print(f"    Items processed in last operation: {items_processed}")
        print(f"    Items failed in last operation: {items_failed}")
        
        if status.last_result.errors:
            print("    Errors in last operation:")
            for error_item in status.last_result.errors:
                error_details = getattr(error_item, 'details', 'N/A')
                doc_key = getattr(error_item, 'key', 'N/A')
                print(f"      - Message: {error_item.message}, Document: {doc_key}, Details: {error_details}")
        else:
            print("    No errors reported in the last operation.")
    else:
        print("  No 'last_result' available (indexer might be initializing or hasn't completed any operation yet).")

    # Detailed execution history (for past completed runs)
    if status.execution_history:
        print("  Recent Execution History (latest first, up to 3):")
        for exec_item in status.execution_history[:3]: # Show latest 3
            run_id = getattr(exec_item, 'id', 'N/A') # Use getattr for safety
            print(f"    -----------------------------------------------------")
            print(f"    - Run Status: {exec_item.status}")
            print(f"      Run ID: {run_id}") # Use the safe run_id
            print(f"      Start: {exec_item.start_time_utc}, End: {exec_item.end_time_utc}")
            print(f"      Items Processed: {exec_item.items_processed}, Items Failed: {exec_item.items_failed}")
            
            if exec_item.errors:
                print("      Errors in this run:")
                for error_item in exec_item.errors[:2]: # Show first 2 errors for brevity
                    error_details_hist = getattr(error_item, 'details', 'N/A')
                    doc_key_hist = getattr(error_item, 'key', 'N/A')
                    print(f"        - {error_item.message} (Doc: {doc_key_hist}, Details: {error_details_hist})")
            else:
                print("      No errors in this run.")
            
            if exec_item.warnings:
                 print("      Warnings in this run:")
                 for warning_item in exec_item.warnings[:2]: # Show first 2 warnings
                    warning_details_hist = getattr(warning_item, 'details', 'N/A')
                    doc_key_warning = getattr(warning_item, 'key', 'N/A')
                    print(f"        - {warning_item.message} (Doc: {doc_key_warning}, Details: {warning_details_hist})")
            else:
                print("      No warnings in this run.")
        print(f"    -----------------------------------------------------")
    else:
        print("  No execution history available.")

except Exception as e:
    print(f"Error running or getting status for indexer '{INDEXER_NAME}': {e}")

print("\nScript completed. Please check the Azure portal for skillset and indexer status and to monitor runs.") 